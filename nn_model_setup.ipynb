{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "50efa414",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import model_utils as mu\n",
    "from gensim.models import Word2Vec\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm.autonotebook import tqdm\n",
    "\n",
    "EMBEDDINGS_SIZE = 50\n",
    "NUM_SEQUENCES_PER_BATCH = 128\n",
    "\n",
    "# Load the pre-trained model\n",
    "sentence_model = SentenceTransformer('all-MiniLM-L6-v2')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "43457e57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>tag</th>\n",
       "      <th>artist</th>\n",
       "      <th>year</th>\n",
       "      <th>views</th>\n",
       "      <th>features</th>\n",
       "      <th>lyrics</th>\n",
       "      <th>id</th>\n",
       "      <th>song</th>\n",
       "      <th>similar_title</th>\n",
       "      <th>similar_artist</th>\n",
       "      <th>similar_song</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Killa Cam</td>\n",
       "      <td>rap</td>\n",
       "      <td>Cam'ron</td>\n",
       "      <td>2004</td>\n",
       "      <td>173166</td>\n",
       "      <td>{\"Cam\\\\'ron\",\"Opera Steve\"}</td>\n",
       "      <td>[Chorus: Opera Steve &amp; Cam'ron]\\nKilla Cam, Ki...</td>\n",
       "      <td>1</td>\n",
       "      <td>Killa Cam by Cam'ron</td>\n",
       "      <td>Bubble Music</td>\n",
       "      <td>Cam'ron</td>\n",
       "      <td>Bubble Music by Cam'ron</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Killa Cam</td>\n",
       "      <td>rap</td>\n",
       "      <td>Cam'ron</td>\n",
       "      <td>2004</td>\n",
       "      <td>173166</td>\n",
       "      <td>{\"Cam\\\\'ron\",\"Opera Steve\"}</td>\n",
       "      <td>[Chorus: Opera Steve &amp; Cam'ron]\\nKilla Cam, Ki...</td>\n",
       "      <td>1</td>\n",
       "      <td>Killa Cam by Cam'ron</td>\n",
       "      <td>Get Down</td>\n",
       "      <td>Cam'ron</td>\n",
       "      <td>Get Down by Cam'ron</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Killa Cam</td>\n",
       "      <td>rap</td>\n",
       "      <td>Cam'ron</td>\n",
       "      <td>2004</td>\n",
       "      <td>173166</td>\n",
       "      <td>{\"Cam\\\\'ron\",\"Opera Steve\"}</td>\n",
       "      <td>[Chorus: Opera Steve &amp; Cam'ron]\\nKilla Cam, Ki...</td>\n",
       "      <td>1</td>\n",
       "      <td>Killa Cam by Cam'ron</td>\n",
       "      <td>The King</td>\n",
       "      <td>Jim Jones</td>\n",
       "      <td>The King by Jim Jones</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Killa Cam</td>\n",
       "      <td>rap</td>\n",
       "      <td>Cam'ron</td>\n",
       "      <td>2004</td>\n",
       "      <td>173166</td>\n",
       "      <td>{\"Cam\\\\'ron\",\"Opera Steve\"}</td>\n",
       "      <td>[Chorus: Opera Steve &amp; Cam'ron]\\nKilla Cam, Ki...</td>\n",
       "      <td>1</td>\n",
       "      <td>Killa Cam by Cam'ron</td>\n",
       "      <td>freestyle</td>\n",
       "      <td>The Diplomats</td>\n",
       "      <td>freestyle by The Diplomats</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Killa Cam</td>\n",
       "      <td>rap</td>\n",
       "      <td>Cam'ron</td>\n",
       "      <td>2004</td>\n",
       "      <td>173166</td>\n",
       "      <td>{\"Cam\\\\'ron\",\"Opera Steve\"}</td>\n",
       "      <td>[Chorus: Opera Steve &amp; Cam'ron]\\nKilla Cam, Ki...</td>\n",
       "      <td>1</td>\n",
       "      <td>Killa Cam by Cam'ron</td>\n",
       "      <td>Santana the great</td>\n",
       "      <td>The Diplomats</td>\n",
       "      <td>Santana the great by The Diplomats</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       title  tag   artist  year   views                     features  \\\n",
       "0  Killa Cam  rap  Cam'ron  2004  173166  {\"Cam\\\\'ron\",\"Opera Steve\"}   \n",
       "1  Killa Cam  rap  Cam'ron  2004  173166  {\"Cam\\\\'ron\",\"Opera Steve\"}   \n",
       "2  Killa Cam  rap  Cam'ron  2004  173166  {\"Cam\\\\'ron\",\"Opera Steve\"}   \n",
       "3  Killa Cam  rap  Cam'ron  2004  173166  {\"Cam\\\\'ron\",\"Opera Steve\"}   \n",
       "4  Killa Cam  rap  Cam'ron  2004  173166  {\"Cam\\\\'ron\",\"Opera Steve\"}   \n",
       "\n",
       "                                              lyrics  id  \\\n",
       "0  [Chorus: Opera Steve & Cam'ron]\\nKilla Cam, Ki...   1   \n",
       "1  [Chorus: Opera Steve & Cam'ron]\\nKilla Cam, Ki...   1   \n",
       "2  [Chorus: Opera Steve & Cam'ron]\\nKilla Cam, Ki...   1   \n",
       "3  [Chorus: Opera Steve & Cam'ron]\\nKilla Cam, Ki...   1   \n",
       "4  [Chorus: Opera Steve & Cam'ron]\\nKilla Cam, Ki...   1   \n",
       "\n",
       "                   song      similar_title similar_artist  \\\n",
       "0  Killa Cam by Cam'ron       Bubble Music        Cam'ron   \n",
       "1  Killa Cam by Cam'ron           Get Down        Cam'ron   \n",
       "2  Killa Cam by Cam'ron           The King      Jim Jones   \n",
       "3  Killa Cam by Cam'ron          freestyle  The Diplomats   \n",
       "4  Killa Cam by Cam'ron  Santana the great  The Diplomats   \n",
       "\n",
       "                         similar_song  \n",
       "0             Bubble Music by Cam'ron  \n",
       "1                 Get Down by Cam'ron  \n",
       "2               The King by Jim Jones  \n",
       "3          freestyle by The Diplomats  \n",
       "4  Santana the great by The Diplomats  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load sentences\n",
    "similar_song_lyrics= pd.read_csv(\"data/processed_data.csv\") \n",
    "\n",
    "similar_song_lyrics.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bdb36d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "lyrics, song_title = similar_song_lyrics[\"lyrics\"].tolist(), similar_song_lyrics[\"similar_title\"].tolist()\n",
    "processed_lyrics = []\n",
    "for lyric in lyrics:\n",
    "    processed_lyrics.append(mu.preprocess_sentence(lyric))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "febe97fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create word embeddings \n",
    "embeddings = sentence_model.encode(processed_lyrics, batch_size=NUM_SEQUENCES_PER_BATCH, convert_to_tensor=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "05aae8c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0408, -0.0494, -0.0330,  ..., -0.0270, -0.0302, -0.0979],\n",
       "        [-0.0408, -0.0494, -0.0330,  ..., -0.0270, -0.0302, -0.0979],\n",
       "        [-0.0408, -0.0494, -0.0330,  ..., -0.0270, -0.0302, -0.0979],\n",
       "        [-0.0408, -0.0494, -0.0330,  ..., -0.0270, -0.0302, -0.0979],\n",
       "        [-0.0408, -0.0494, -0.0330,  ..., -0.0270, -0.0302, -0.0979]],\n",
       "       device='mps:0')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7e7e913e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Fit and transform the labels to integers\n",
    "y_encoded = label_encoder.fit_transform(song_title)\n",
    "\n",
    "# Convert to a PyTorch tensor\n",
    "y_tensor = torch.tensor(y_encoded, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cfa5b11b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#word_dataloader_train, word_dataloader_test = mu.create_dataloaders(embeddings, y_tensor, NUM_SEQUENCES_PER_BATCH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "425b8b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10 points\n",
    "\n",
    "class FFNN(nn.Module):\n",
    "    \"\"\"\n",
    "    A class representing our implementation of a Feed-Forward Neural Network.\n",
    "    You will need to implement two methods:\n",
    "        - A constructor to set up the architecture and hyperparameters of the model\n",
    "        - The forward pass\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size: int, embedding_size: int, hidden_units=128, device: str = \"mps\"):\n",
    "        \"\"\"\n",
    "        Initialize a new untrained model. \n",
    "        \n",
    "        You can change these parameters as you would like.\n",
    "        Once you get a working model, you are encouraged to\n",
    "        experiment with this constructor to improve performance.\n",
    "        \n",
    "        Params:\n",
    "            vocab_size: The number of words in the vocabulary\n",
    "            ngram: The value of N for training and prediction.\n",
    "            embedding_layer: The previously trained embedder. \n",
    "            hidden_units: The size of the hidden layer.\n",
    "        \"\"\"        \n",
    "        super().__init__()\n",
    "        # YOUR CODE HERE\n",
    "        # we recommend saving the parameters as instance variables\n",
    "        # so you can access them later as needed\n",
    "        # (in addition to anything else you need to do here)\n",
    "        \n",
    "\t\t# Saving parameters as instance variables\n",
    "        self.vocab_size = vocab_size\n",
    "        #self.ngram = ngram\n",
    "        self.hidden_units = hidden_units\n",
    "         # Embedding size\n",
    "\t\t# Save embedding size\n",
    "\n",
    "        #embedding_size = embedding_layer.embedding_dim\n",
    "        \n",
    "\t\t# Defining layers\n",
    "        self.flatten = nn.Flatten() # Useful later to flatten array of ngram-1 after embedding before passing it to the linear layer\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "\t\t\tnn.Linear(in_features=embedding_size, out_features=hidden_units, bias=True),\n",
    "\t\t\tnn.ReLU(),\n",
    "\t\t\tnn.Linear(in_features=hidden_units, out_features=vocab_size, bias=True)\n",
    "\t\t)\n",
    "\n",
    "        self.to(device)\n",
    "        \n",
    "    def forward(self, X: list) -> torch.tensor:\n",
    "        \"\"\"\n",
    "        Compute the forward pass through the network.\n",
    "        This is not a prediction, and it should not apply softmax.\n",
    "\n",
    "        Params:\n",
    "            X: the input data\n",
    "\n",
    "        Returns:\n",
    "            The output of the model; i.e. its predictions.\n",
    "        \n",
    "        \"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        flat_embedded = self.flatten(X)\n",
    "        logits = self.linear_relu_stack(flat_embedded)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "51db96d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"mps\") if torch.backends.mps.is_available() else torch.device(\"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "469ce224",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10 points\n",
    "\n",
    "# Defining a training function that goes over every batch per epoch\n",
    "def train_one_epoch(dataloader, nn_model, optimizer, loss_fn):\n",
    "    epoch_loss = 0\n",
    "\n",
    "    for data in dataloader:\n",
    "        # Separating the input + label pair for each instance\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "\t\t# Zeroing gradients for every batch\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "\t\t# Make predictions for this batch\n",
    "        outputs = nn_model(inputs)\n",
    "        \n",
    "\t\t# Compute loss and gradients\n",
    "        batch_loss = loss_fn(outputs, labels)\n",
    "        batch_loss.backward()\n",
    "        \n",
    "\t\t# Adjust learning weights\n",
    "        optimizer.step()\n",
    "        \n",
    "\t\t# Adding to epoch loss\n",
    "        epoch_loss += batch_loss.item() # Covert scalar tensor into floating-point\n",
    "\n",
    "    return epoch_loss\n",
    "\n",
    "# Defining a general training function that goes over all the epochs\n",
    "def train(dataloader, input_model, epochs: int = 1, lr: float = 0.001) -> None:\n",
    "    \"\"\"\n",
    "    Our model's training loop.\n",
    "    Print the cross entropy loss every epoch.\n",
    "    You should use the Adam optimizer instead of SGD.\n",
    "\n",
    "    When looking for documentation, try to stay on PyTorch's website.\n",
    "    This might be a good place to start: https://pytorch.org/tutorials/beginner/introyt/trainingyt.html \n",
    "    They should have plenty of tutorials, and we don't want you to get confused from other resources.\n",
    "\n",
    "    Params:\n",
    "        dataloader: The training dataloader\n",
    "        model: The model we wish to train\n",
    "        epochs: The number of epochs to train for\n",
    "        lr: Learning rate \n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    # you will need to initialize an optimizer and a loss function, which you should do\n",
    "    # before the training loop\n",
    "    \n",
    "    optimizer = torch.optim.Adam(input_model.parameters(), lr=lr) # Adam optimizer instead of SGD\n",
    "    loss_fn = torch.nn.CrossEntropyLoss() # Multinomial Cross Entropy Loss that applies log-softmax internally and computes the negative log likelihood\n",
    "    \n",
    "    n_batches = len(dataloader)\n",
    "    \n",
    "\t# Making sure gradient tracking is on before start training\n",
    "    input_model.train()\n",
    "    \n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        epoch_loss = train_one_epoch(dataloader, input_model, optimizer, loss_fn)\n",
    "        avg_epoch_loss = epoch_loss/n_batches\n",
    "        print(f\"Epoch: {epoch}, Loss: {avg_epoch_loss}\\n\")\n",
    "\n",
    "    # print out the epoch number and the current average loss after each epoch\n",
    "    # you can use tqdm to print out a progress bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8b4516f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FFNN(\n",
       "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "  (linear_relu_stack): Sequential(\n",
       "    (0): Linear(in_features=384, out_features=128, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=128, out_features=637, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn_model = FFNN(vocab_size=len(song_title) , embedding_size=384)\n",
    "nn_model.to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ecf75a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train(word_dataloader_train, nn_model, epochs=5, lr=0.01)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c1ba1dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3 points\n",
    "\n",
    "# make a function that does your full *training* pipeline\n",
    "# This is essentially pulling the pieces that you've done so far earlier in this \n",
    "# notebook into a single function that you can call to train your model\n",
    "\n",
    "def full_pipeline(x,y,\n",
    "                batch_size:int = NUM_SEQUENCES_PER_BATCH, hidden_units = 128, epochs = 1,\n",
    "                lr = 0.001, test_pct = 0.1\n",
    "                ) -> FFNN:\n",
    "    \"\"\"\n",
    "    Run the entire pipeline from loading embeddings to training.\n",
    "    You won't use the test set for anything.\n",
    "\n",
    "    Params:\n",
    "        data: The raw data to train on, parsed as a list of lists of tokens\n",
    "        word_embeddings_filename: The filename of the Word2Vec word embeddings\n",
    "        batch_size: The batch size to use\n",
    "        hidden_units: The number of hidden units to use\n",
    "        epochs: The number of epochs to train for\n",
    "        lr: The learning rate to use\n",
    "        test_pct: The proportion of samples to use in the test set.\n",
    "\n",
    "    Returns:\n",
    "        The trained model.\n",
    "    \"\"\"\n",
    "    # Loading embeddings\n",
    "    \n",
    "\t# Define vocab size from embedder\n",
    "    vocab_size = len(y)\n",
    "    \n",
    "\t# Prepare training samples\n",
    "    #X = torch.tensor(embeddings, dtype=torch.float32)\n",
    "\n",
    "    x_train, x_test, y_train, y_test = mu.split_dataset(x, y)\n",
    "\n",
    "\t# Create training dataloader\n",
    "    dataloader_train, dataloader_test = mu.create_dataloaders(x_train, x_test, y_train,y_test, num_sequences_per_batch=NUM_SEQUENCES_PER_BATCH)\n",
    "\n",
    "\t# Create FFNN model\n",
    "    nn_model = FFNN(vocab_size=vocab_size, embedding_size=384, hidden_units=hidden_units)\n",
    "\n",
    "\t# Train our model\n",
    "    train(dataloader=dataloader_train, input_model=nn_model, epochs=epochs, lr=lr)\n",
    "\n",
    "    return nn_model, dataloader_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "07974da3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([637, 384])\n"
     ]
    }
   ],
   "source": [
    "print(embeddings.shape)  # Should be [num_samples, embedding_dim]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9beaf22e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a04d6328a674007a8bf8a32b685bcbb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 6.4593600034713745\n",
      "\n",
      "Epoch: 1, Loss: 6.441712498664856\n",
      "\n",
      "Epoch: 2, Loss: 6.421833038330078\n",
      "\n",
      "Epoch: 3, Loss: 6.394532680511475\n",
      "\n",
      "Epoch: 4, Loss: 6.357338905334473\n",
      "\n",
      "Epoch: 5, Loss: 6.311632990837097\n",
      "\n",
      "Epoch: 6, Loss: 6.253365635871887\n",
      "\n",
      "Epoch: 7, Loss: 6.188197612762451\n",
      "\n",
      "Epoch: 8, Loss: 6.118694424629211\n",
      "\n",
      "Epoch: 9, Loss: 6.0501827001571655\n",
      "\n",
      "Epoch: 10, Loss: 5.987354636192322\n",
      "\n",
      "Epoch: 11, Loss: 5.935549259185791\n",
      "\n",
      "Epoch: 12, Loss: 5.89809513092041\n",
      "\n",
      "Epoch: 13, Loss: 5.872668623924255\n",
      "\n",
      "Epoch: 14, Loss: 5.859583139419556\n",
      "\n",
      "Epoch: 15, Loss: 5.849327445030212\n",
      "\n",
      "Epoch: 16, Loss: 5.836706042289734\n",
      "\n",
      "Epoch: 17, Loss: 5.823588967323303\n",
      "\n",
      "Epoch: 18, Loss: 5.811398386955261\n",
      "\n",
      "Epoch: 19, Loss: 5.800655007362366\n",
      "\n",
      "Epoch: 20, Loss: 5.7902034521102905\n",
      "\n",
      "Epoch: 21, Loss: 5.780754327774048\n",
      "\n",
      "Epoch: 22, Loss: 5.7702943086624146\n",
      "\n",
      "Epoch: 23, Loss: 5.759566783905029\n",
      "\n",
      "Epoch: 24, Loss: 5.748237729072571\n",
      "\n",
      "Epoch: 25, Loss: 5.737322449684143\n",
      "\n",
      "Epoch: 26, Loss: 5.724811553955078\n",
      "\n",
      "Epoch: 27, Loss: 5.713632822036743\n",
      "\n",
      "Epoch: 28, Loss: 5.700116872787476\n",
      "\n",
      "Epoch: 29, Loss: 5.686180949211121\n",
      "\n",
      "Epoch: 30, Loss: 5.672043323516846\n",
      "\n",
      "Epoch: 31, Loss: 5.657207369804382\n",
      "\n",
      "Epoch: 32, Loss: 5.6420063972473145\n",
      "\n",
      "Epoch: 33, Loss: 5.625899076461792\n",
      "\n",
      "Epoch: 34, Loss: 5.609183311462402\n",
      "\n",
      "Epoch: 35, Loss: 5.591709136962891\n",
      "\n",
      "Epoch: 36, Loss: 5.573293924331665\n",
      "\n",
      "Epoch: 37, Loss: 5.554485559463501\n",
      "\n",
      "Epoch: 38, Loss: 5.5337419509887695\n",
      "\n",
      "Epoch: 39, Loss: 5.513782858848572\n",
      "\n",
      "Epoch: 40, Loss: 5.492034316062927\n",
      "\n",
      "Epoch: 41, Loss: 5.470021724700928\n",
      "\n",
      "Epoch: 42, Loss: 5.446646571159363\n",
      "\n",
      "Epoch: 43, Loss: 5.423088312149048\n",
      "\n",
      "Epoch: 44, Loss: 5.398215651512146\n",
      "\n",
      "Epoch: 45, Loss: 5.373029112815857\n",
      "\n",
      "Epoch: 46, Loss: 5.346841096878052\n",
      "\n",
      "Epoch: 47, Loss: 5.319668531417847\n",
      "\n",
      "Epoch: 48, Loss: 5.291465163230896\n",
      "\n",
      "Epoch: 49, Loss: 5.262735486030579\n",
      "\n",
      "Epoch: 50, Loss: 5.233895659446716\n",
      "\n",
      "Epoch: 51, Loss: 5.203403830528259\n",
      "\n",
      "Epoch: 52, Loss: 5.172676205635071\n",
      "\n",
      "Epoch: 53, Loss: 5.140684604644775\n",
      "\n",
      "Epoch: 54, Loss: 5.10768723487854\n",
      "\n",
      "Epoch: 55, Loss: 5.074787616729736\n",
      "\n",
      "Epoch: 56, Loss: 5.041589856147766\n",
      "\n",
      "Epoch: 57, Loss: 5.006787180900574\n",
      "\n",
      "Epoch: 58, Loss: 4.972521901130676\n",
      "\n",
      "Epoch: 59, Loss: 4.93644917011261\n",
      "\n",
      "Epoch: 60, Loss: 4.900202751159668\n",
      "\n",
      "Epoch: 61, Loss: 4.8637001514434814\n",
      "\n",
      "Epoch: 62, Loss: 4.826900482177734\n",
      "\n",
      "Epoch: 63, Loss: 4.789453983306885\n",
      "\n",
      "Epoch: 64, Loss: 4.750838994979858\n",
      "\n",
      "Epoch: 65, Loss: 4.712605834007263\n",
      "\n",
      "Epoch: 66, Loss: 4.673912286758423\n",
      "\n",
      "Epoch: 67, Loss: 4.635293006896973\n",
      "\n",
      "Epoch: 68, Loss: 4.595712304115295\n",
      "\n",
      "Epoch: 69, Loss: 4.556715965270996\n",
      "\n",
      "Epoch: 70, Loss: 4.517003297805786\n",
      "\n",
      "Epoch: 71, Loss: 4.476915001869202\n",
      "\n",
      "Epoch: 72, Loss: 4.4368427991867065\n",
      "\n",
      "Epoch: 73, Loss: 4.397481918334961\n",
      "\n",
      "Epoch: 74, Loss: 4.357462644577026\n",
      "\n",
      "Epoch: 75, Loss: 4.316659927368164\n",
      "\n",
      "Epoch: 76, Loss: 4.27654504776001\n",
      "\n",
      "Epoch: 77, Loss: 4.236769080162048\n",
      "\n",
      "Epoch: 78, Loss: 4.1967527866363525\n",
      "\n",
      "Epoch: 79, Loss: 4.156151473522186\n",
      "\n",
      "Epoch: 80, Loss: 4.116560935974121\n",
      "\n",
      "Epoch: 81, Loss: 4.076365232467651\n",
      "\n",
      "Epoch: 82, Loss: 4.037653148174286\n",
      "\n",
      "Epoch: 83, Loss: 3.998037874698639\n",
      "\n",
      "Epoch: 84, Loss: 3.9591826796531677\n",
      "\n",
      "Epoch: 85, Loss: 3.9202016592025757\n",
      "\n",
      "Epoch: 86, Loss: 3.881224811077118\n",
      "\n",
      "Epoch: 87, Loss: 3.842841923236847\n",
      "\n",
      "Epoch: 88, Loss: 3.80540132522583\n",
      "\n",
      "Epoch: 89, Loss: 3.7681738138198853\n",
      "\n",
      "Epoch: 90, Loss: 3.730621039867401\n",
      "\n",
      "Epoch: 91, Loss: 3.693533480167389\n",
      "\n",
      "Epoch: 92, Loss: 3.657822608947754\n",
      "\n",
      "Epoch: 93, Loss: 3.622285783290863\n",
      "\n",
      "Epoch: 94, Loss: 3.5866532921791077\n",
      "\n",
      "Epoch: 95, Loss: 3.5516883730888367\n",
      "\n",
      "Epoch: 96, Loss: 3.516509473323822\n",
      "\n",
      "Epoch: 97, Loss: 3.4825679063796997\n",
      "\n",
      "Epoch: 98, Loss: 3.4490362405776978\n",
      "\n",
      "Epoch: 99, Loss: 3.4159661531448364\n",
      "\n",
      "Epoch: 100, Loss: 3.38357812166214\n",
      "\n",
      "Epoch: 101, Loss: 3.351458251476288\n",
      "\n",
      "Epoch: 102, Loss: 3.3200013041496277\n",
      "\n",
      "Epoch: 103, Loss: 3.2891429662704468\n",
      "\n",
      "Epoch: 104, Loss: 3.2590381503105164\n",
      "\n",
      "Epoch: 105, Loss: 3.2296087741851807\n",
      "\n",
      "Epoch: 106, Loss: 3.199869155883789\n",
      "\n",
      "Epoch: 107, Loss: 3.170895576477051\n",
      "\n",
      "Epoch: 108, Loss: 3.143296778202057\n",
      "\n",
      "Epoch: 109, Loss: 3.115575909614563\n",
      "\n",
      "Epoch: 110, Loss: 3.08859521150589\n",
      "\n",
      "Epoch: 111, Loss: 3.0614947080612183\n",
      "\n",
      "Epoch: 112, Loss: 3.0364052057266235\n",
      "\n",
      "Epoch: 113, Loss: 3.0110164284706116\n",
      "\n",
      "Epoch: 114, Loss: 2.9854124784469604\n",
      "\n",
      "Epoch: 115, Loss: 2.9615648984909058\n",
      "\n",
      "Epoch: 116, Loss: 2.93808376789093\n",
      "\n",
      "Epoch: 117, Loss: 2.91536682844162\n",
      "\n",
      "Epoch: 118, Loss: 2.892410695552826\n",
      "\n",
      "Epoch: 119, Loss: 2.8712123036384583\n",
      "\n",
      "Epoch: 120, Loss: 2.849014639854431\n",
      "\n",
      "Epoch: 121, Loss: 2.828874170780182\n",
      "\n",
      "Epoch: 122, Loss: 2.8074912428855896\n",
      "\n",
      "Epoch: 123, Loss: 2.7865607142448425\n",
      "\n",
      "Epoch: 124, Loss: 2.7665104269981384\n",
      "\n",
      "Epoch: 125, Loss: 2.748413383960724\n",
      "\n",
      "Epoch: 126, Loss: 2.72998309135437\n",
      "\n",
      "Epoch: 127, Loss: 2.7122949957847595\n",
      "\n",
      "Epoch: 128, Loss: 2.69462388753891\n",
      "\n",
      "Epoch: 129, Loss: 2.6767192482948303\n",
      "\n",
      "Epoch: 130, Loss: 2.660229980945587\n",
      "\n",
      "Epoch: 131, Loss: 2.643540918827057\n",
      "\n",
      "Epoch: 132, Loss: 2.6277960538864136\n",
      "\n",
      "Epoch: 133, Loss: 2.6119275093078613\n",
      "\n",
      "Epoch: 134, Loss: 2.5968191027641296\n",
      "\n",
      "Epoch: 135, Loss: 2.582387089729309\n",
      "\n",
      "Epoch: 136, Loss: 2.567886710166931\n",
      "\n",
      "Epoch: 137, Loss: 2.55390065908432\n",
      "\n",
      "Epoch: 138, Loss: 2.5406410098075867\n",
      "\n",
      "Epoch: 139, Loss: 2.5268372297286987\n",
      "\n",
      "Epoch: 140, Loss: 2.5145386457443237\n",
      "\n",
      "Epoch: 141, Loss: 2.5024081468582153\n",
      "\n",
      "Epoch: 142, Loss: 2.4893179535865784\n",
      "\n",
      "Epoch: 143, Loss: 2.4769601821899414\n",
      "\n",
      "Epoch: 144, Loss: 2.4650319814682007\n",
      "\n",
      "Epoch: 145, Loss: 2.45427805185318\n",
      "\n",
      "Epoch: 146, Loss: 2.443409562110901\n",
      "\n",
      "Epoch: 147, Loss: 2.433027148246765\n",
      "\n",
      "Epoch: 148, Loss: 2.423230528831482\n",
      "\n",
      "Epoch: 149, Loss: 2.4120424389839172\n",
      "\n",
      "Epoch: 150, Loss: 2.40290766954422\n",
      "\n",
      "Epoch: 151, Loss: 2.3933075666427612\n",
      "\n",
      "Epoch: 152, Loss: 2.3835856318473816\n",
      "\n",
      "Epoch: 153, Loss: 2.3744847178459167\n",
      "\n",
      "Epoch: 154, Loss: 2.3653514981269836\n",
      "\n",
      "Epoch: 155, Loss: 2.3575921058654785\n",
      "\n",
      "Epoch: 156, Loss: 2.3491955399513245\n",
      "\n",
      "Epoch: 157, Loss: 2.340972363948822\n",
      "\n",
      "Epoch: 158, Loss: 2.3324074745178223\n",
      "\n",
      "Epoch: 159, Loss: 2.3249815702438354\n",
      "\n",
      "Epoch: 160, Loss: 2.3178536891937256\n",
      "\n",
      "Epoch: 161, Loss: 2.310257315635681\n",
      "\n",
      "Epoch: 162, Loss: 2.3031692504882812\n",
      "\n",
      "Epoch: 163, Loss: 2.296288788318634\n",
      "\n",
      "Epoch: 164, Loss: 2.2902064323425293\n",
      "\n",
      "Epoch: 165, Loss: 2.2822651863098145\n",
      "\n",
      "Epoch: 166, Loss: 2.2768682837486267\n",
      "\n",
      "Epoch: 167, Loss: 2.269592046737671\n",
      "\n",
      "Epoch: 168, Loss: 2.264364778995514\n",
      "\n",
      "Epoch: 169, Loss: 2.258634090423584\n",
      "\n",
      "Epoch: 170, Loss: 2.252442240715027\n",
      "\n",
      "Epoch: 171, Loss: 2.2468998432159424\n",
      "\n",
      "Epoch: 172, Loss: 2.241100490093231\n",
      "\n",
      "Epoch: 173, Loss: 2.2358739376068115\n",
      "\n",
      "Epoch: 174, Loss: 2.2301352620124817\n",
      "\n",
      "Epoch: 175, Loss: 2.224848210811615\n",
      "\n",
      "Epoch: 176, Loss: 2.2213229537010193\n",
      "\n",
      "Epoch: 177, Loss: 2.215614140033722\n",
      "\n",
      "Epoch: 178, Loss: 2.21073716878891\n",
      "\n",
      "Epoch: 179, Loss: 2.206627130508423\n",
      "\n",
      "Epoch: 180, Loss: 2.2016491889953613\n",
      "\n",
      "Epoch: 181, Loss: 2.196528375148773\n",
      "\n",
      "Epoch: 182, Loss: 2.1935247778892517\n",
      "\n",
      "Epoch: 183, Loss: 2.189265489578247\n",
      "\n",
      "Epoch: 184, Loss: 2.1845261454582214\n",
      "\n",
      "Epoch: 185, Loss: 2.1819653511047363\n",
      "\n",
      "Epoch: 186, Loss: 2.1771528720855713\n",
      "\n",
      "Epoch: 187, Loss: 2.1735647916793823\n",
      "\n",
      "Epoch: 188, Loss: 2.169763922691345\n",
      "\n",
      "Epoch: 189, Loss: 2.1656463742256165\n",
      "\n",
      "Epoch: 190, Loss: 2.1627199053764343\n",
      "\n",
      "Epoch: 191, Loss: 2.1593615412712097\n",
      "\n",
      "Epoch: 192, Loss: 2.155158042907715\n",
      "\n",
      "Epoch: 193, Loss: 2.1513651609420776\n",
      "\n",
      "Epoch: 194, Loss: 2.149668335914612\n",
      "\n",
      "Epoch: 195, Loss: 2.1459792256355286\n",
      "\n",
      "Epoch: 196, Loss: 2.1432909965515137\n",
      "\n",
      "Epoch: 197, Loss: 2.1403368711471558\n",
      "\n",
      "Epoch: 198, Loss: 2.1374685168266296\n",
      "\n",
      "Epoch: 199, Loss: 2.1343958377838135\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nn_model, test_dataloader = full_pipeline(x=embeddings, y=y_tensor, epochs=200)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fb947b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_model.eval()  # Set the model to evaluation mode\n",
    "nn_model.to(\"cpu\")  # Move the model to CPU for inference if needed\n",
    "# use the model to classify test data set\n",
    "def evaluate_model(model, dataloader):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in dataloader:\n",
    "            inputs, labels = inputs.to(\"cpu\"), labels.to(\"cpu\")  # Move to CPU if needed\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = correct / total\n",
    "    print(f'Accuracy of the model on the test set: {accuracy * 100:.2f}%')\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b2f20d58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the model on the test set: 3.97%\n",
      "Model saved as 'trained_ffnn_model.pth'\n",
      "0.03968253968253968\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the test set\n",
    "accuracy = evaluate_model(nn_model, test_dataloader)\n",
    "# Save the trained model\n",
    "torch.save(nn_model.state_dict(), \"trained_ffnn_model.pth\")\n",
    "print(\"Model saved as 'trained_ffnn_model.pth'\")\n",
    "# Load the model for future use\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e73eba0",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
